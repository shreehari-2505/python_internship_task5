# ğŸŒ³ Decision Trees & Random Forest â€“ Heart Disease Prediction

## ğŸ“Œ Objective

This project is part of my **AI & ML Internship â€“ Task 5**.  
The goal is to **build, visualize, and evaluate Decision Tree and Random Forest classifiers** to predict the presence of heart disease using the **Heart Disease dataset**.

---

## ğŸ›  Tools & Libraries Used

|Tool / Library|Purpose|
|---|---|
|**Python**|Core programming language|
|**Pandas**|Data loading, cleaning, wrangling|
|**NumPy**|Numerical computations|
|**Matplotlib / Seaborn**|Data visualization|
|**Scikit-learn**|Model building, preprocessing, evaluation|
|**Graphviz**|Decision tree visualization|
|**Google Colab**|Notebook execution environment|

---

## ğŸ”„ Workflow â€“ Step-by-Step Logic Flow

[Start]  
â†“  
**Load dataset** (`pd.read_csv`)  
â†“  
**Exploratory Data Analysis (EDA)** â†’ shape, preview, missing values heatmap, target distribution  
â†“  
**Preprocessing** â†’ Label encoding for categorical features, feature scaling with `StandardScaler`  
â†“  
**Split dataset** into Train/Test sets with stratified sampling  
â†“  
**Decision Tree Classifier** â†’ Train, visualize, tune `max_depth` to control overfitting  
â†“  
**Accuracy vs Depth Analysis** â†’ plot training & testing accuracy to pick best depth  
â†“  
**Random Forest Classifier** â†’ Train and compare accuracy with Decision Tree  
â†“  
**Feature Importance** â†’ Extract and plot top contributing features  
â†“  
**Cross-validation** â†’ Evaluate model robustness  
â†“  
[End]

---

## ğŸ§ª Steps Performed in Detail

### 1ï¸âƒ£ Data Loading & EDA

- Dataset: **Heart Disease Dataset** (Kaggle)
    
- Checked shape, column info, statistical summary
    
- Visualized missing values heatmap
    
- Countplot for target distribution (heart disease presence vs absence)
    

### 2ï¸âƒ£ Preprocessing

- **Label Encoding** â†’ Converted categorical features to numeric form using `LabelEncoder`
    
- **Feature Scaling** â†’ Applied `StandardScaler` to normalize all features except target
    

### 3ï¸âƒ£ Splitting Data

- **Train/Test Split** â†’ 80% training, 20% testing with `stratify=y` to maintain class balance
    

### 4ï¸âƒ£ Decision Tree Model

- Trained a baseline `DecisionTreeClassifier`
    
- Tuned `max_depth` from 1 to 20
    
- Plotted **Accuracy vs Tree Depth** to visualize overfitting/underfitting
    
- Selected optimal depth with highest test accuracy
    
- Visualized final tree with `plot_tree`
    

### 5ï¸âƒ£ Random Forest Model

- Trained `RandomForestClassifier`
    
- Compared accuracy with Decision Tree
    
- Extracted **feature importances** and plotted them using `sns.barplot`
    

### 6ï¸âƒ£ Model Evaluation

- Used accuracy score for both models
    
- Performed 5-fold **Cross Validation** to assess generalization
    

---

## ğŸ“š Vocabulary of Functions & Commands Used

|Command / Function|Purpose|
|---|---|
|`pd.read_csv(path)`|Load CSV dataset|
|`df.info()`|Overview of columns, types, and missing values|
|`sns.heatmap()`|Visualize missing data|
|`LabelEncoder()`|Encode categorical variables|
|`StandardScaler()`|Scale numerical features|
|`train_test_split()`|Split dataset into training and testing sets|
|`DecisionTreeClassifier()`|Initialize Decision Tree model|
|`.fit(X_train, y_train)`|Train model|
|`.score(X, y)`|Compute accuracy|
|`RandomForestClassifier()`|Initialize Random Forest model|
|`.feature_importances_`|Get importance score for each feature|
|`cross_val_score()`|Perform k-fold cross-validation|
|`plot_tree()`|Visualize decision tree|

---

## ğŸ“Š Key Insights

- **Decision Trees** can overfit at high depths â€” best performance found at depth â‰ˆ optimal value from tuning.
    
- **Random Forests** generally outperform single trees due to bagging and averaging predictions.
    
- Feature importance analysis shows which health metrics most strongly influence heart disease predictions
